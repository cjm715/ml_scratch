\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}


\renewcommand{\vec}[1]{
	\mathbf{#1}
}


\title{Theory of Na\"ive Bayes}
\date{\today}
\author{Chris Miles}

\begin{document}
	
	\maketitle
	
	\section{Overview}

	Suppose you have a dictionary of words which are indexed in alphabetical order. Let $w_i$ be the $i$th word in the dictionary. We are interested in creating a feature vector $\vec{x}$ which has elements $x_i$ where $x_i = 1$ if $w_i$ is in the document or $x_i = 0$ if $w_i$ in the document. We are interested in classifying a document with the probability:
	
	\[p(\vec{x}| y) = p(x_1, x_2, \dots, x_{10000} | y) =  p(x_1 | y)p(x_2 | y) \dots p(x_n|y)\]
		
	The above expression for the conditional joint probability is not true in general. The core idea of Na\"ive Bayes is to make the assumption that the conditional probability of seeing a word $w_i$ and the $w_j$ only depends on the class $y$.
\end{document}