{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/SMS_SPAM/SMSSpamCollection.csv', sep = '\\t', header=None)\n",
    "df = df.sample(frac=1, axis=1).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "split_frac = 0.8\n",
    "split_idx = round(split_frac * df.shape[0])\n",
    "\n",
    "df_train = df.iloc[:split_idx, :]\n",
    "df_test = df.iloc[split_idx:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to create the features from text. we are interested in createing \n",
    "# a dictionary of words. Requires splitting on space and removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "doc_counter = {}\n",
    "doc_counter['ham'] = Counter()\n",
    "doc_counter['spam'] = Counter()\n",
    "for label in ['ham', 'spam']:\n",
    "    for i in range(df_train.shape[0]):\n",
    "        curr_label = df_train.loc[i, 0]\n",
    "        if curr_label == label:\n",
    "            example = df_train.loc[i, 1]\n",
    "            words = example.split(' ')\n",
    "            words=[re.sub(r'[^\\w\\s]','', word) for word in words]\n",
    "            doc_counter[label] = doc_counter[label] + Counter(list(set(words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.99999882, ..., 1.        , 1.        ,\n",
       "       0.99999685])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df_train.iloc[:,0]\n",
    "n = {}\n",
    "n['ham'] = sum(y_train == 'ham') # number of documents that are ham\n",
    "n['spam'] = sum(y_train == 'spam') # number of documents taht are spam\n",
    "n_docs= n['ham']  +n['spam'] \n",
    "prior = {}\n",
    "prior['ham'] = n['ham'] /n_docs\n",
    "prior['spam'] = n['spam']/n_docs\n",
    "num_words = len(doc_counter['ham'] + doc_counter['spam'])\n",
    "\n",
    "prob_ham_test = np.zeros(shape = df_test.shape[0])\n",
    "for i in range(df_test.shape[0]):\n",
    "    example = df_test.iloc[i, 1]\n",
    "    words = example.split(' ')\n",
    "    words = [re.sub(r'[^\\w\\s]','', word) for word in words]\n",
    "    words = list(set(words))\n",
    "    \n",
    "    prob = {}\n",
    "    for label in ['ham', 'spam']:\n",
    "        likelihood = 1\n",
    "        for word in words:\n",
    "            prob_word_given_label = (doc_counter[label][word]+1)/(n[label] + num_words)\n",
    "            likelihood = likelihood * prob_word_given_label\n",
    "        prob[label] = likelihood * prior[label]\n",
    "    \n",
    "    prob_ham_scaled = prob['ham']/(prob['ham'] + prob['spam'])\n",
    "    prob_ham_test[i] = prob_ham_scaled\n",
    "prob_ham_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = df_test.iloc[:,0]\n",
    "is_ham_test =  np.array(1*(y_test == 'ham'))\n",
    "is_ham_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "TP = sum(is_ham_test * (prob_ham_test > threshold))\n",
    "FP = sum(np.logical_not(is_ham_test) * (prob_ham_test > threshold))\n",
    "TN = sum(np.logical_not(is_ham_test) * (prob_ham_test < threshold))\n",
    "FN = sum(is_ham_test * (prob_ham_test < threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[113,  38],\n",
       "       [  2, 961]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "# rows are labels\n",
    "# cols are predictions\n",
    "\n",
    "C = np.array([[TN, FP],\n",
    "              [FN, TP]])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619619619619619"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = TP / (TP + FP)\n",
    "precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979231568016614"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = TP / (TP + FN)\n",
    "recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
